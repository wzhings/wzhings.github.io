<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Overview on Zuhui Wang&#39;s Homepage</title>
    <link>https://wzhings.github.io/tutorial/</link>
    <description>Recent content in Overview on Zuhui Wang&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;{year} Zuhui Wang</copyright>
    <lastBuildDate>Sun, 09 Sep 2018 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="https://wzhings.github.io/tutorial/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Equation derivation procedure for 1.90</title>
      <link>https://wzhings.github.io/tutorial/prml_ch1/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0500</pubDate>
      
      <guid>https://wzhings.github.io/tutorial/prml_ch1/</guid>
      <description>Section 1.5.5 in Pattern Recognition and Machine Learning (PRML) In this note, I will show the derivation procedure of the equation of 1.90 step by step. Before we start, I want to claim that the version of the textbook are 2006 version. The Eq. 1.90 is list on Page 47.
From Page 46 we know, the expected loss can be written as
\[\small \mathbb{E}[L] = \iint\{y(\mathbf{x}) - t\}^2 p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t \]</description>
    </item>
    
  </channel>
</rss>