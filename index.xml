<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zuhui Wang&#39;s Homepage</title>
    <link>https://wzhings.github.io/</link>
    <description>Recent content on Zuhui Wang&#39;s Homepage</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy;{year} Zuhui Wang</copyright>
    <lastBuildDate>Sun, 09 Sep 2018 00:00:00 -0500</lastBuildDate>
    
	    <atom:link href="https://wzhings.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Academic</title>
      <link>https://wzhings.github.io/other_homes/hero/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://wzhings.github.io/other_homes/hero/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The Best Way to Create the Website You Want from Markdown (or RStudio/Jupyter)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Build &lt;strong&gt;Anything&lt;/strong&gt; with Widgets&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;text-shadow: none;&#34;&gt;&lt;a class=&#34;github-button&#34; href=&#34;https://github.com/gcushen/hugo-academic&#34; data-icon=&#34;octicon-star&#34; data-size=&#34;large&#34; data-show-count=&#34;true&#34; aria-label=&#34;Star this on GitHub&#34;&gt;Star&lt;/a&gt;&lt;script async defer src=&#34;https://buttons.github.io/buttons.js&#34;&gt;&lt;/script&gt;&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posts</title>
      <link>https://wzhings.github.io/other_homes/posts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://wzhings.github.io/other_homes/posts/</guid>
      <description>&lt;p&gt;Null&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experience</title>
      <link>https://wzhings.github.io/other_homes/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://wzhings.github.io/other_homes/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accomplish&amp;shy;ments</title>
      <link>https://wzhings.github.io/other_homes/accomplishments/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://wzhings.github.io/other_homes/accomplishments/</guid>
      <description></description>
    </item>
    
    <item>
      <title>People</title>
      <link>https://wzhings.github.io/other_homes/people/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://wzhings.github.io/other_homes/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent &amp; Upcoming Talks</title>
      <link>https://wzhings.github.io/other_homes/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://wzhings.github.io/other_homes/talks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Featured Publications</title>
      <link>https://wzhings.github.io/other_homes/featured/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://wzhings.github.io/other_homes/featured/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Popular Topics</title>
      <link>https://wzhings.github.io/other_homes/tags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://wzhings.github.io/other_homes/tags/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Equation derivation procedure for 1.90</title>
      <link>https://wzhings.github.io/tutorial/prml_ch1/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0500</pubDate>
      
      <guid>https://wzhings.github.io/tutorial/prml_ch1/</guid>
      <description>

&lt;!--
inline \\(a^2 + b^2 = c\\)
block: \\[ \\]
--&gt;

&lt;h3 id=&#34;section-1-5-5-in-pattern-recognition-and-machine-learning-prml&#34;&gt;Section 1.5.5 in Pattern Recognition and Machine Learning (PRML)&lt;/h3&gt;

&lt;p&gt;In this note, I will show the derivation procedure of the equation of 1.90 step by step. Before we start, I want to claim that the version of the textbook are 2006 version. The Eq. 1.90 is list on Page 47.&lt;/p&gt;

&lt;p&gt;From Page 46 we know, the expected loss can be written as&lt;/p&gt;

&lt;p&gt;\[\small
\mathbb{E}[L] = \iint\{y(\mathbf{x}) - t\}^2 p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t
\]&lt;/p&gt;

&lt;p&gt;And we know&lt;/p&gt;

&lt;p&gt;\[\small
\{y(\mathbf{x}) - t\}^2 = \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}] + \mathbb{E}[t|\mathbf{x}] - t\}^2 = \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}^2 + 2\{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}\{\mathbb{E}[t|\mathbf{x}] - t\} + \{\mathbb{E}[t|\mathbf{x}] - t\}^2
\]
In the above equation,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for the first term \(\small \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}^2\), we put it back to the expected loss equation and get
\[\small
\iint \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}^2 p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t = \int \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}^2 p(\mathbf{x})\mathrm{d}\mathbf{x}
\]
Because we know the marginal probability is \(\int p(\mathbf{x}, t)\mathrm{d}t = p(\mathbf x)\), and \(\small \mathbb{E}[t|\mathbf{x}] \) is a &lt;em&gt;constant&lt;/em&gt; when \(\mathbf x\) is given.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;for the middle term, \(\small 2\{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}\{\mathbb{E}[t|\mathbf{x}] - t\}\), we can also put it back to the expected loss equaiton. Here we omit the constant number &amp;ndash; $\small 2$ and get
$$ \small
\iint \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}\{\mathbb{E}[t|\mathbf{x}] - t\} p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t = \iint \big( \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\} \mathbb{E}[t|\mathbf{x}] - \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}t\big) p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t $$
$$ \small
= \iint \big( \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\} \mathbb{E}[t|\mathbf{x}] \big)p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t - \iint \big(\{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}t \big)p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t
$$
In the first term of the above equation, we know $\small \mathbb{E}[t|\mathbf{x}]$ is a constant. Then, the &lt;strong&gt;&lt;em&gt;first term&lt;/em&gt;&lt;/strong&gt; can be written as:
$$\small
\iint \big( \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\} \mathbb{E}[t|\mathbf{x}] \big)p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t = \int \big(\{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\})\mathbb{E}[t|\mathbf{x}] \}p(\mathbf x) \mathrm d\mathbf x&lt;br /&gt;
$$
The &lt;strong&gt;&lt;em&gt;second term&lt;/em&gt;&lt;/strong&gt; in the above equaiton can be written as (with joing probability definition $\small p(\mathbf x, t)$):
$$ \small
\iint \big(\{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}t \big)p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t = \iint \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}t p(t|\mathbf x)p(\mathbf x)\mathrm{d}\mathbf{x}\mathrm{d}t
$$
We also know the defintion of $\small \mathbb E[t|\mathbf x] = \int t p(t|\mathbf x) dt$, then the above equation can be written as:
$$\small
\iint \big(\{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}t \big)p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t= \iint \big(\{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}\big)t p(t|\mathbf x)p(\mathbf x)\mathrm{d}\mathbf{x}\mathrm{d}t = \int \big(\{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\})\mathbb{E}[t|\mathbf{x}]p(\mathbf x) \mathrm d\mathbf x
$$
Then, we know the &lt;strong&gt;&lt;em&gt;first term&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;second term&lt;/em&gt;&lt;/strong&gt; are idetical. Therefore, the middle term of the original euqation is just $\small 0$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;for the last term, $\small \{\mathbb{E}[t|\mathbf{x}] - t\}^2$ can be insert back to the expected loss euqation and get
$$\small
\iint \{\mathbb{E}[t|\mathbf{x}] - t\}^2p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t = \iint \{t - \mathbb{E}[t|\mathbf{x}]\}^2 p(t|\mathbf x)p(\mathbf x)\mathrm dt \mathrm d\mathbf x
$$
Here we need to use another time of &lt;em&gt;Expectation&lt;/em&gt; definition:
$$\small
\int \{\mathbb{E}[t|\mathbf{x}] - t\}^2p(t|\mathbf x)\mathrm dt = \mathbb E[\{t - \mathbb{E}[t|\mathbf{x}]\}^2|\mathbf x] = \mathrm{var}[t|\mathbf x]
$$
Then, the last term can be written as following:
$$\small
\iint \{\mathbb{E}[t|\mathbf{x}] - t\}^2p(\mathbf{x}, t)\mathrm{d}\mathbf{x}\mathrm{d}t = \iint \{t - \mathbb{E}[t|\mathbf{x}]\}^2 p(t|\mathbf x)p(\mathbf x)\mathrm dt \mathrm d\mathbf x = \int \mathrm{var}[t|\mathbf x] p(\mathbf x) \mathrm d\mathbf x
$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, to get the expected loss equaiton of $1.90$, we can have
$$\small
\mathbb{E}[L] = \int \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}^2 p(\mathbf{x})\mathrm{d}\mathbf{x} +  \int \mathrm{var}[t|\mathbf x] p(\mathbf x) \mathrm d\mathbf x
$$&lt;/p&gt;

&lt;p&gt;BTW, for the 2006 version E-book, the last term of Eq.$1.90$ has a typo. It has been fixed at page 7 in the &lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/prml-errata-1st-20110921.pdf&#34; target=&#34;_blank&#34;&gt;PRML Errata File&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;-[&lt;strong&gt;END&lt;/strong&gt;]-&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
